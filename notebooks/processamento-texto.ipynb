{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "processamento-texto.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "I76qmkHvwnDz",
        "colab_type": "text"
      },
      "source": [
        "## Introdução\n",
        "Conforme detalhei na Seção 4.2.3 da minha [monografia](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/documents/tcc-monografia-ajb.pdf), na etapa de processamento de texto do meu trabalho, defini 4 _pipelines_ distintos, sendo eles:\n",
        "\n",
        "1. Aplicação de _stemming_, representando textos por _Bag-of-Words_;\n",
        "2. Aplicação de _stemming_, representando textos por TF-IDF;\n",
        "3. Aplicação de _lematização_, representando textos por _Bag-of-Words_; e\n",
        "4. Aplicação de _lematização_, representando textos por TF-IDF.\n",
        "\n",
        "A ordem de aplicação dos métodos pode ser vista abaixo, para melhor compreensão:\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/JxkJ377/fluxo.png\" width=\"350\"></center>\n",
        "\n",
        "Neste Notebook, vou demonstrar os códigos que utilizei para cada método, para caso alguém esteja aprendendo Python e/ou processamento de linguagem natural e queira saber como fiz.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LNHecetnwnD8",
        "colab_type": "text"
      },
      "source": [
        "## Etapas Básicas\n",
        "Não importa qual _pipeline_ estejamos usando, todos eles têm 3 etapas em comum: conversão para caixa baixa, remoção de pontuação e de _stop words_.\n",
        "\n",
        "Para demonstrar o funcionamento destas três, usaremos o seguinte exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "kKhX5OIPwnEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'HelLo, wOrLd! How aRe YoU?'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9V7MFsiownER",
        "colab_type": "text"
      },
      "source": [
        "### Caixa Baixa\n",
        "Mesmo sendo uma etapa simples, é importante para que o classificador não entenda duas grafias de uma mesma palavra como sendo duas palavras distintas (por exemplo, `caixa` e `Caixa`).\n",
        "\n",
        "A própria biblioteca String tem um método para fazer isso facilmente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MDEptWEdwnEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd7f389b-3bc7-4895-93d8-bd1e56dface6"
      },
      "source": [
        "import string\n",
        "\n",
        "lowercase = sentence.lower()\n",
        "\n",
        "print(lowercase)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello, world! how are you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RLzMSaMJwnEi",
        "colab_type": "text"
      },
      "source": [
        "### Remoção de Pontuações\n",
        "As pontuações não serão úteis para o aprendizado dos classificadores usados no trabalho, então todas são retiradas.\n",
        "\n",
        "Especialmente para um dos _datasets_ que utilizei ([Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)), foi necessário incrementar esta etapa, removendo também _tags_ HTML que estavam presentes nos textos.\n",
        "\n",
        "Com o auxílio dos métodos `translate()` e `maketrans()`, substituí todas as pontuações por espaços em branco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "e3xAYMXqwnEm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d57bf69-b803-4387-fd9a-19b22c7d6159"
      },
      "source": [
        "tags_removed = lowercase.replace('<br />', ' ')\n",
        "punctuation_removed = tags_removed.translate(tags_removed.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
        "                                             \n",
        "print(punctuation_removed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello  world  how are you \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tb81T2TvwnEx",
        "colab_type": "text"
      },
      "source": [
        "### Remoção de _Stop Words_\n",
        "_Stop words_ são palavras que, sozinhas, não possuem significado. Elas sempre precisam estar acompanhadas de outras palavras para fazerem sentido. Alguns exemplos são artigos e preposições.\n",
        "\n",
        "A biblioteca NLTK traz uma lista de _stop words_ para diversos idiomas (incluindo Português), que usamos para fazer a remoção delas de nossos conjuntos de dados, após separar cada texto em tokens. No meu trabalho, utilizei o conjunto de _stop words_ em Inglês."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yhgd_u4NwnE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "a666c020-64eb-4aa8-9dff-e2d575b3de9c"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "tokens = word_tokenize(punctuation_removed)\n",
        "list_stopwords = set(stopwords.words('english'))\n",
        "stopwords_removed = [word for word in tokens if word not in list_stopwords]\n",
        "\n",
        "print(stopwords_removed)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['hello', 'world']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eUh60MhswnE_",
        "colab_type": "text"
      },
      "source": [
        "Até este momento, os textos ficarão em forma de lista de tokens, para que sejam processadas posteriormente com a técnica escolhida. A partir daqui, os quatro _pipelines_ se dividem.\n",
        "\n",
        "---\n",
        "## Redução para Formas Básicas\n",
        "Com o tratamento dos textos feito até aqui, a próxima etapa envolve a redução de todas as palavras do texto para alguma forma básica delas. Existem duas técnicas conhecidas, o _stemming_ e a lematização, que são aplicadas separadamente.\n",
        "\n",
        "### _Stemming_\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8Zu0M5xGwnFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO implementação do stemming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "luprD6ZDwnFJ",
        "colab_type": "text"
      },
      "source": [
        "### Lematização\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gxkaiCqPwnFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO implementação da lematização"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FhG3gkJdwnFX",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Representação dos Textos\n",
        "Por fim, para que o algoritmo de _Machine Learning_ consiga interpretar, extrair padrões do nosso conjunto de dados e gerar um modelo dele, é preciso que os textos sejam representados numericamente.\n",
        "\n",
        "Dessa forma, utilizei _Bag-of-Words_ (BoW) e TF-IDF para fazer essa conversão.\n",
        "\n",
        "### _Bag-of-Words_\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ffnr3H77wnFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO implementação do BoW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Kiur_DaxwnFh",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wi028DmjwnFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO implementação direta do TF-IDF, com TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2f8p3K30wnFs",
        "colab_type": "text"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XjFDdWYUwnFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO implementação do TF-IDF, em cima do Bag-of-Words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KyBFbh3lwnF4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "Com isso, encerra-se a implementação das técnicas de processamento de texto aplicadas no meu trabalho.\n",
        "\n",
        "Para fins de modularização, para os experimentos do meu TCC, reuni todas estas técnicas em uma única classe Python, importando-a em meus Scripts dos modelos. Esta classe pode ser encontrada [aqui](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/models/text_processing.py).\n",
        "\n",
        "Os códigos do meu trabalho (principalmente dos métodos demonstrados neste Notebook) ainda podem ser bastante melhorados. Por exemplo, um meio que estudei após a apresentação do meu trabalho envolve usar a classe `sklearn.pipeline.Pipeline`. Futuramente, farei uma nova versão dessas implementações do meu TCC."
      ]
    }
  ]
}