{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I76qmkHvwnDz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introdução\n",
    "Conforme detalhei na Seção 4.2.3 da minha [monografia](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/documents/tcc-monografia-ajb.pdf), na etapa de processamento de texto do meu trabalho, defini 4 _pipelines_ distintos, sendo eles:\n",
    "\n",
    "1. Aplicação de _stemming_, representando textos por _Bag-of-Words_;\n",
    "2. Aplicação de _stemming_, representando textos por TF-IDF;\n",
    "3. Aplicação de _lematização_, representando textos por _Bag-of-Words_; e\n",
    "4. Aplicação de _lematização_, representando textos por TF-IDF.\n",
    "\n",
    "A ordem de aplicação dos métodos pode ser vista abaixo, para melhor compreensão:\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/JxkJ377/fluxo.png\" width=\"350\"></center>\n",
    "\n",
    "Neste Notebook, vou demonstrar os códigos que utilizei para cada método, para caso alguém esteja aprendendo Python e/ou processamento de linguagem natural e queira saber como fiz.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNHecetnwnD8",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Etapas Básicas\n",
    "Não importa qual _pipeline_ estejamos usando, todos eles têm 3 etapas em comum: conversão para caixa baixa, remoção de pontuação e de _stop words_.\n",
    "\n",
    "Para demonstrar o funcionamento destas três, usaremos o seguinte exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKhX5OIPwnEA",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'JoHN lIkEs to WaTch MOviEs. MaRy LiKes mOvIeS ToO. LoOK, MARy aLso LikEs tO wAtCH fOoTbaLl gAmEs!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9V7MFsiownER"
   },
   "source": [
    "### Caixa Baixa\n",
    "Mesmo sendo uma etapa simples, é importante para que o classificador não entenda duas grafias de uma mesma palavra como sendo duas palavras distintas (por exemplo, `caixa` e `Caixa`).\n",
    "\n",
    "A própria biblioteca String tem um método para fazer isso facilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MDEptWEdwnEV",
    "outputId": "dd7f389b-3bc7-4895-93d8-bd1e56dface6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john likes to watch movies. mary likes movies too. look, mary also likes to watch football games!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "lowercase = sentence.lower()\n",
    "\n",
    "print(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLzMSaMJwnEi"
   },
   "source": [
    "### Remoção de Pontuações\n",
    "As pontuações não serão úteis para o aprendizado dos classificadores usados no trabalho, então todas são retiradas.\n",
    "\n",
    "Especialmente para um dos _datasets_ que utilizei ([Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)), foi necessário incrementar esta etapa, removendo também _tags_ HTML que estavam presentes nos textos.\n",
    "\n",
    "Com o auxílio dos métodos `translate()` e `maketrans()`, substituí todas as pontuações por espaços em branco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e3xAYMXqwnEm",
    "outputId": "6d57bf69-b803-4387-fd9a-19b22c7d6159",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john likes to watch movies  mary likes movies too  look  mary also likes to watch football games \n"
     ]
    }
   ],
   "source": [
    "tags_removed = lowercase.replace('<br />', ' ')\n",
    "punctuation_removed = tags_removed.translate(tags_removed.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                                             \n",
    "print(punctuation_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tb81T2TvwnEx"
   },
   "source": [
    "### Remoção de _Stop Words_\n",
    "_Stop words_ são palavras que, sozinhas, não possuem significado. Elas sempre precisam estar acompanhadas de outras palavras para fazerem sentido. Alguns exemplos são artigos e preposições.\n",
    "\n",
    "A biblioteca NLTK traz uma lista de _stop words_ para diversos idiomas (incluindo Português), que usamos para fazer a remoção delas de nossos conjuntos de dados, após separar cada texto em tokens. No meu trabalho, utilizei o conjunto de _stop words_ em Inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "yhgd_u4NwnE0",
    "outputId": "a666c020-64eb-4aa8-9dff-e2d575b3de9c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'likes', 'watch', 'movies', 'mary', 'likes', 'movies', 'look', 'mary', 'also', 'likes', 'watch', 'football', 'games']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokens = word_tokenize(punctuation_removed)\n",
    "list_stopwords = set(stopwords.words('english'))\n",
    "stopwords_removed = [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "print(stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUh60MhswnE_"
   },
   "source": [
    "Até este momento, os textos ficarão em forma de lista de tokens, para que sejam processadas posteriormente com a técnica escolhida. A partir daqui, os quatro _pipelines_ se dividem.\n",
    "\n",
    "---\n",
    "## Redução das Palavras\n",
    "Com o tratamento dos textos feito até aqui, a próxima etapa envolve a redução da inflexão de todas as palavras do texto. Existem duas técnicas conhecidas, o _stemming_ e a lematização, que são aplicadas separadamente.\n",
    "\n",
    "### _Stemming_\n",
    "O processo de _stemming_, em resumo, retira o sufixo da palavra e reduz ela ao seu radical.\n",
    "\n",
    "Há vários algoritmos conhecidos para esta etapa, sendo um deles o algoritmo de Porter, que usei no meu trabalho. Ele já vem implementado na biblioteca NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Zu0M5xGwnFB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john like watch movi mari like movi look mari also like watch footbal game\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in stopwords_removed]\n",
    "stemmed_sentence = \" \".join(stemmed_words)\n",
    "\n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luprD6ZDwnFJ"
   },
   "source": [
    "### Lematização\n",
    "Já a lematização reduz as palavras à sua forma mais básica. Por exemplo, \"vendido\" é reduzido ao infinitivo, \"vender\".\n",
    "\n",
    "A implementação aqui é um pouco mais complexa, pois para fazer a lematização, é necessária uma etapa a mais, de _Part-of-Speech Tagging_ (ou _POS Tagging_). Conforme descrevi na minha monografia (Seção 2.2.1.5), cada palavra recebe uma _tag_ morfossintática, que indica sua classificação dentro do contexto da frase. Somente após a atribuição dessas _tags_, podemos começar a lematização.\n",
    "\n",
    "Um exemplo de _POS Tagging_ pode ser visto na figura abaixo.\n",
    "\n",
    "<center><img src=\"https://d33wubrfki0l68.cloudfront.net/d5cbc4b0e14c20f877366b69b9171649afe11fda/d96a8/assets/images/bigram-hmm/pos-title.jpg\" width=\"512\"></center>\n",
    "\n",
    "A biblioteca NLTK também contém um lematizador, mas que funciona com WordNet, um banco de dados lexical que também opera com suas próprias _tags_. De modo que possamos usar este lematizador, é preciso mapear as _tags_ do POS para as do WordNet.\n",
    "\n",
    "Para isso, definimos o método a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxkaiCqPwnFL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_wordnet(pos_tags):\n",
    "    if pos_tags.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tags.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tags.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tags.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, realizamos a etapa de _POS Tagging_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "pos_tagging = pos_tag(stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as _tags_ atribuídas, fazemos o mapeamento para as _tags_ do WordNet, usando o método `pos_wordnet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_tags = map(lambda x: (x[0], pos_wordnet(x[1])), pos_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, pode-se realizar a lematização em si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john like watch movie mary like movie look mary also like watch football game\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word, tag in lemmatization_tags:\n",
    "    if tag is None:\n",
    "        lemmatized_words.append(word)\n",
    "    else:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhG3gkJdwnFX"
   },
   "source": [
    "---\n",
    "## Representação dos Textos\n",
    "Por fim, para que o algoritmo de _Machine Learning_ consiga interpretar, extrair padrões do nosso conjunto de dados e gerar um modelo dele, é preciso que os textos sejam representados numericamente.\n",
    "\n",
    "Dessa forma, utilizei _Bag-of-Words_ (BoW) e TF-IDF para fazer essa conversão.\n",
    "\n",
    "Para melhor exemplificação, usaremos 3 exemplos para demonstrar os dois métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [\n",
    "    'John likes to watch movies. Mary likes movies too.',\n",
    "    'Mary also likes to watch football games.',\n",
    "    'John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "É importante ressaltar que não aplicarei as etapas de processamento que mostrei até aqui, dessa vez. O intuito de usar estes três exemplos acima é puramente gerar uma melhor visualização da ideia das técnicas de representação."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Bag-of-Words_\n",
    "No BoW, para cada texto, gera-se um vetor cujo tamanho equivale à quantidade de palavras contidas em todos os textos do _dataset_. Cada posição deste vetor representa uma palavra, e seu valor é dado pelo número de vezes que esse termo aparece no texto.\n",
    "\n",
    "Para esse mapeamento do texto para os vetores, usei a biblioteca Scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "bag_of_words = vectorizer_bow.fit_transform(data).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A lista completa de palavras (ou atributos) fica da seguinte forma:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(vectorizer_bow.get_feature_names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "E então, o vetor gerado por _Bag-of-Words_ fica desta forma:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bag_of_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF\n",
    "Já o TF-IDF, por sua vez, calcula uma frequência relativa para cada palavra, por meio de uma proporção inversa entre sua frequência absoluta em um dado texto e a porcentagem de documentos em que este termo aparece (ver Seção 2.2.3.2 da monografia).\n",
    "\n",
    "Esta representação também é implementada pelo Scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tf_idf = TfidfVectorizer()\n",
    "tf_idf = vectorizer_tf_idf.fit_transform(data).toarray()\n",
    "\n",
    "print(tf_idf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note que usamos a frequência absoluta de cada palavra para calcular o TF-IDF, exatamente o que é utilizado para gerar o _Bag-of-Words_. Para situações específicas, pode ser interessante gerar o BoW primeiro, para depois transformar o vetor para a representação por TF-IDF. Isto pode ser feito com a classe `TfidfTransformer`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer_tf_idf = TfidfTransformer()\n",
    "transformation = transformer_tf_idf.fit_transform(bag_of_words).toarray()\n",
    "\n",
    "print(transformation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Com isso, encerra-se a implementação das técnicas de processamento de texto aplicadas no meu trabalho. Estes vetores demonstrados acima são, então, usados nos algoritmos de classificação, para treino dos respectivos modelos.\n",
    "\n",
    "Para fins de modularização, reuni todas estas técnicas em uma única classe Python, importando-a em meus Scripts dos modelos, para os experimentos do meu TCC. Esta classe pode ser encontrada [aqui](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/models/text_processing.py).\n",
    "\n",
    "Os códigos do meu trabalho (principalmente dos métodos demonstrados neste Notebook) ainda podem ser bastante melhorados. Por exemplo, um meio que estudei após a apresentação do meu trabalho, para organizar toda a sequência de aplicação das técnicas de processamento, envolve usar a classe `sklearn.pipeline.Pipeline`. Futuramente, farei uma nova versão dessas implementações do meu TCC, com essa e outras melhorias."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "Com isso, encerra-se a implementação das técnicas de processamento de texto aplicadas no meu trabalho. Estes vetores demonstrados acima são, então, usados nos algoritmos de classificação, para treino dos respectivos modelos.\n",
    "\n",
    "Para fins de modularização, reuni todas estas técnicas em uma única classe Python, importando-a em meus Scripts dos modelos, para os experimentos do meu TCC. Esta classe pode ser encontrada [aqui](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/models/text_processing.py).\n",
    "\n",
    "Os códigos do meu trabalho (principalmente dos métodos demonstrados neste Notebook) ainda podem ser bastante melhorados. Por exemplo, um meio que estudei após a apresentação do meu trabalho, para organizar toda a sequência de aplicação das técnicas de processamento, envolve usar a classe `sklearn.pipeline.Pipeline`. Futuramente, farei uma nova versão dessas implementações do meu TCC, com essa e outras melhorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante ressaltar que não aplicarei as etapas de processamento que mostrei até aqui, dessa vez. O intuito de usar estes três exemplos acima é puramente gerar uma melhor visualização da ideia das técnicas de representação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Bag-of-Words_\n",
    "No BoW, para cada texto, gera-se um vetor cujo tamanho equivale à quantidade de palavras contidas em todos os textos do _dataset_. Cada posição deste vetor representa uma palavra, e seu valor é dado pelo número de vezes que esse termo aparece no texto.\n",
    "\n",
    "Para esse mapeamento do texto para os vetores, usei a biblioteca Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ffnr3H77wnFY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "bag_of_words = vectorizer_bow.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lista completa de palavras (ou atributos) fica da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então, o vetor gerado por _Bag-of-Words_ fica desta forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 2 1 2 1 1 1]\n",
      " [1 1 1 0 1 1 0 1 0 1]\n",
      " [1 1 1 1 3 2 2 2 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kiur_DaxwnFh"
   },
   "source": [
    "### TF-IDF\n",
    "Já o TF-IDF, por sua vez, calcula uma frequência relativa para cada palavra, por meio de uma proporção inversa entre sua frequência absoluta em um dado texto e a porcentagem de documentos em que este termo aparece (ver Seção 2.2.3.2 da monografia).\n",
    "\n",
    "Esta representação também é implementada pelo Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi028DmjwnFk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.3127806  0.48580407 0.24290204\n",
      "  0.62556119 0.24290204 0.3127806  0.24290204]\n",
      " [0.42983971 0.42983971 0.42983971 0.         0.33380888 0.33380888\n",
      "  0.         0.33380888 0.         0.33380888]\n",
      " [0.21484319 0.21484319 0.21484319 0.21484319 0.5005347  0.3336898\n",
      "  0.42968638 0.3336898  0.21484319 0.3336898 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tf_idf = TfidfVectorizer()\n",
    "tf_idf = vectorizer_tf_idf.fit_transform(data).toarray()\n",
    "\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2f8p3K30wnFs"
   },
   "source": [
    "Note que usamos a frequência absoluta de cada palavra para calcular o TF-IDF, exatamente o que é utilizado para gerar o _Bag-of-Words_. Para situações específicas, pode ser interessante gerar o BoW primeiro, para depois transformar o vetor para a representação por TF-IDF. Isto pode ser feito com a classe `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjFDdWYUwnFw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.3127806  0.48580407 0.24290204\n",
      "  0.62556119 0.24290204 0.3127806  0.24290204]\n",
      " [0.42983971 0.42983971 0.42983971 0.         0.33380888 0.33380888\n",
      "  0.         0.33380888 0.         0.33380888]\n",
      " [0.21484319 0.21484319 0.21484319 0.21484319 0.5005347  0.3336898\n",
      "  0.42968638 0.3336898  0.21484319 0.3336898 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer_tf_idf = TfidfTransformer()\n",
    "transformation = transformer_tf_idf.fit_transform(bag_of_words).toarray()\n",
    "\n",
    "print(transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyBFbh3lwnF4"
   },
   "source": [
    "---\n",
    "\n",
    "Com isso, encerra-se a implementação das técnicas de processamento de texto aplicadas no meu trabalho. Estes vetores demonstrados acima são, então, usados nos algoritmos de classificação, para treino dos respectivos modelos.\n",
    "\n",
    "Para fins de modularização, reuni todas estas técnicas em uma única classe Python, importando-a em meus Scripts dos modelos, para os experimentos do meu TCC. Esta classe pode ser encontrada [aqui](https://github.com/anajbellini/SentimentAnalysis-MovieReviews/blob/master/models/text_processing.py).\n",
    "\n",
    "Os códigos do meu trabalho (principalmente dos métodos demonstrados neste Notebook) ainda podem ser bastante melhorados. Por exemplo, um meio que estudei após a apresentação do meu trabalho, para organizar toda a sequência de aplicação das técnicas de processamento, envolve usar a classe `sklearn.pipeline.Pipeline`. Futuramente, farei uma nova versão dessas implementações do meu TCC, com essa e outras melhorias."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "processamento-texto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}